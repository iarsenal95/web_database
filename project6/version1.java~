package edu.umich.cse.eecs485;

import java.io.IOException;

import org.apache.hadoop.conf.Configuration;
import org.apache.hadoop.fs.Path;
import org.apache.hadoop.io.LongWritable;
import org.apache.hadoop.io.Text;
import org.apache.hadoop.mapreduce.Job;
import org.apache.hadoop.mapreduce.Mapper;
import org.apache.hadoop.mapreduce.Reducer;
import org.apache.hadoop.mapreduce.lib.input.FileInputFormat;
import org.apache.hadoop.mapreduce.lib.output.FileOutputFormat;
import org.apache.hadoop.mapreduce.lib.output.TextOutputFormat;
import org.apache.mahout.classifier.bayes.XmlInputFormat;
import java.util.*;
import nu.xom.*;
//import java.util.Iterator;
//import java.util.regex.Matcher;
//import java.util.regex.Pattern;

//import java.util.Hashtable;
import java.io.FileReader;
import java.io.BufferedReader;
//import java.util.Set;
//import java.util.Map;
import java.lang.Math;


public class InvertedIndex
{
	private static N = 0;
	private static HashSet<String> stopwords = new HashSet<String>();

	public static class tfMap extends Mapper<LongWritable, Text, Text, Text> {
		public void map(LongWritable key, Text value, Context context)
				throws IOException, InterruptedException {

			String strId = "";
			String strBody = "";

			// Parse the xml and read data (page id and article body)
			// Using XOM library
			Builder builder = new Builder();

			try {
//				System.out.println(value.toString());
				Document doc = builder.build(value.toString(), null);

				Nodes nodeId = doc.query("//eecs485_article_id");
				strId = nodeId.get(0).getChild(0).getValue();
				
				Nodes nodeBody = doc.query("//eecs485_article_body");
				strBody = nodeBody.get(0).getChild(0).getValue();
			}
			// indicates a well-formedness error
			catch (ParsingException ex) { 
				System.out.println("Not well-formed.");
				System.out.println(ex.getMessage());
			}  
			catch (IOException ex) {
				System.out.println("io exception");
			}
			
			// update N
			N++;

			// Tokenize document body
			Pattern pattern = Pattern.compile("\\w+");
			Matcher matcher = pattern.matcher(strBody);
			
			Hashtable<String, Integer> termfreq = new Hashtable<String, Integer>();
			while (matcher.find()){
				// Write the parsed token
				// check if this term is in stopwords, if not, update the termfreq
				String term = matcher.group();
				if (!stopwords.contains(term)){
					if (termfreq.containsKey(term)){
						termfreq.put(term, termfreq.get(term)+1)
					}
					else
						termfreq.put(term, 1);
				}
			}
			Set set = termfreq.entrySet();
			Iterator it = set.iterator();
			while (it.hasNext()){
      			Map.Entry entry = (Map.Entry) it.next();
				String s = strId + " " + Integer.toString(entry.getValue());
				context.write(new Text(entry.getKey()), new Text(s));
			}
		}
	}

	public static class tfReduce extends Reducer<Text, Text, Text, Text> {
		
		public void reduce(Text key, Iterable<Text> values, Context context)
				throws IOException, InterruptedException {
			
			double n = 0;
			it = values.iterator();
			while(it.hasNext){
				it.next();
				n++;
			}
			double itf = log(N/n);
			String doclist = "";
			
			for (Text value : values){
				StringTokenizer tokenizer = new StringTokenizer(values.get());
				String docid = tokenizer.nextToken();
				String termfreq = tokenizer.nextToken();
				double tf = Double.parseDouble(termfreq)*itf;
				doclist  = docid + ":" + String.valueOf(tf) + " ";
			}
			context.write(key, new Text(result));
		}
	}

	public static class normMap extends Mapper<LongWritable, Text, Text, LongWritable> {
		public void map(LongWritable key, Text value, Context context)
				throws IOException, InterruptedException {

			
		}
	}

	public static class normReduce extends Reducer<Text, LongWritable, Text, Text> {
		
		public void reduce(Text key, Iterable<LongWritable> values, Context context)
				throws IOException, InterruptedException {
			
		}
	}


	public static class invIndexMap extends Mapper<LongWritable, Text, Text, LongWritable> {
		public void map(LongWritable key, Text value, Context context)
				throws IOException, InterruptedException {

			
		}
	}

	public static class invIndexReduce extends Reducer<Text, LongWritable, Text, Text> {
		
		public void reduce(Text key, Iterable<LongWritable> values, Context context)
				throws IOException, InterruptedException {
			
		}
	}


	private void iniStopwords(String stopWordsPath){
		BufferedReader inputStream = null;
		try{
    		inputStream = new BufferedReader(new FileReader(stopWordsPath));
			String l;
        	while ((l = inputStream.readLine()) != null)
			{
				stopwords.add(l);
        	}
    	}
		finally
		{
    		if (inputStream != null)
			{
        		inputStream.close();
        	}
		}
	}

	// ?? how the file is split
	// term docid tf; docid tf;.... 
	public static void normMapReduce(String inputPath, String outputPath, String stopWordsPath)
	{
		Configuration conf = new Configuration();

		Job job = new Job(conf, "getNorm");

		job.setOutputKeyClass(Text.class);
		job.setOutputValueClass(LongWritable.class);

		job.setMapperClass(normMap.class);
		job.setReducerClass(normReduce.class);

		job.setInputFormatClass(TextInputFormat.class);
		job.setOutputFormatClass(TextOutputFormat.class);

		FileInputFormat.addInputPath(job, new Path(inputPath));
		FileOutputFormat.setOutputPath(job, new Path(outputPath));

		job.waitForCompletion(true);
	}

	public static void tfMapReduce(String inputPath, String outputPath)
	{
		iniStopwords(stopWordsPath);		
		Configuration conf = new Configuration();

		conf.set("xmlinput.start", "<eecs485_article>");
		conf.set("xmlinput.end", "</eecs485_article>");

		Job job = new Job(conf, "getTf");

		job.setOutputKeyClass(Text.class);
		job.setOutputValueClass(LongWritable.class);

		job.setMapperClass(tfMap.class);
		job.setReducerClass(tfReduce.class);

		job.setInputFormatClass(XmlInputFormat.class);
		job.setOutputFormatClass(TextOutputFormat.class);

		FileInputFormat.addInputPath(job, new Path(inputPath));
		FileOutputFormat.setOutputPath(job, new Path(outputPath));

		job.waitForCompletion(true);
	}

	public static void invIndexMapReduce(String inputPath, String outputPath)
	{
		Configuration conf = new Configuration();

		Job job = new Job(conf, "getInvIndex");

		job.setOutputKeyClass(Text.class);
		job.setOutputValueClass(LongWritable.class);

		job.setMapperClass(invIndexMap.class);
		job.setReducerClass(invIndexReduce.class);

		job.setInputFormatClass(TextInputFormat.class);
		job.setOutputFormatClass(TextOutputFormat.class);

		FileInputFormat.addInputPath(job, new Path(inputPath));
		FileOutputFormat.setOutputPath(job, new Path(outputPath));

		job.waitForCompletion(true);
	}
	
	// args[0] input xml file
	// args[1] output file
	// args[2] stopwords
	public static void main(String[] args) throws Exception
	{
		String tfInputPath = args[0];
		String tfOutputPath = args[1];
		//String normOutputPath = "";
		//String invIndexOutputPath = args[1];
		String stopWordsPath = args[2];
		
		tfMapReduce(tfInputPath, tfOutputPath, stopWordsPath);
		//normMapReduce(tfOutputPath, normOutputPath);
		//invIndexMapReduce(tfOutputPath, invIndexOutputPath);
	}
}
























